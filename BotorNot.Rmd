---
title: "Botornot"
author: "Malte SÃ¶hren"
date: "10/16/2020"
output: html_document
---
# Loading Packages 

```{r}
library(tidyverse)
library(DataExplorer)
library(caret)
library(caretEnsemble)
library(pROC)
library(gbm)
library(data.table)
library(superml)
library(keras)
```

```{r}
setwd("/Users/2egaa/Desktop/nG-BotClassifier")
data <- read.csv("botornot-1.csv")
```

# Data Cleaning 

Identify relevant variables on theoretical note 

```{r}
data_base <-data %>%  select(c(bot, overallTime, consentTime, readProfileTime, readEssayTime, speeder, superspeeder, quirk, age, birthday, birthmonth, gender, totlanguage, god, grownup_us, samestate, sametown, incomeclass, retake, session, disconnected, treatment, feedback, grownup_us, parentsdivorced, children, pets.0, military, gayfriends, incomebracket, income, session))
```

Recode NA's in bot column as zeros 

```{r}
data_base$bot[is.na(data_base$bot)] <- 0
data_base$bot[data_base$bot == 2] <- 1
data_base$bot[data_base$bot == 3] <- 1

data_base$bot <- as.factor(data_base$bot)

data_base$bot <- factor(data_base$bot, levels=c(0,1), labels=c("No", "Yes"))
data_base$speeder <- as.factor(data_base$speeder)
data_base$superspeeder <- as.factor(data_base$superspeeder)
data_base$retake <- as.factor(data_base$retake)
```

Remove NA Rows

```{r}
data_clean <- data_base %>% na.omit()
```

Count words in quirk variable

```{r}
data_clean <- data_clean %>% mutate(quirk_count = sapply(gregexpr("\\S+", data$quirk), length), feedback_count = sapply(gregexpr("\\S+", data_clean$feedback), length))

```

# Short summary

```{r}
data_clean %>% group_by(bot) %>% 
         summarize(overalltime = median(overallTime),
                   consenttime = median(consentTime), 
                   readProfileTime = median(readProfileTime), 
                   readEssayTime = median(readEssayTime), 
                   speeder = mean(as.numeric(speeder)), 
                   superspeeder = mean(superspeeder), 
                   quirk_count = mean(quirk_count),
                   age = median(age))
```

# Plotting differences

```{r}
ggplot(data_clean, aes(x = bot, y = consentTime)) + geom_boxplot(outlier.shape = NA) + scale_y_continuous(limits = quantile(data$consentTime, c(0.1, 0.9)))

plotdata <- data_clean %>% select(c(bot, consentTime, readProfileTime, readEssayTime, quirk_count, feedback_count, age))

df.m <- melt(plotdata, id.var = "bot", factorsAsStrings=F)

ggplot(data = df.m, aes(x=variable, y=value)) + geom_boxplot(aes(fill=bot)) + scale_y_continuous(limits = quantile(plotdata$consentTime, c(0.1, 0.9)))
```

# Bag of Words

```{r}
cv <- CountVectorizer$new(max_features = 300, remove_stopwords = FALSE, lowercase = TRUE)

bow <- cv$fit_transform(data_clean$feedback)

bow <- as.data.frame(bow)

modeldata <- bind_cols(data_clean, bow)
modeldata <- modeldata %>% select(-c(quirk, feedback...23))

colnames(modeldata) <- make.names(colnames(modeldata))
```

# Setting up modeling 

Split data into train & test set. Here, a 70/30 split is used to provide the test set with some more cases of the minority class (80/20-split would have resulted in only 14 cases which the model can be tested on)

```{r}
set.seed(92381)

cdp <- createDataPartition(modeldata$bot, 
                               p = .7, 
                               list = FALSE, 
                               times = 1)

model_train <- modeldata[cdp,]
model_test <- modeldata[-cdp,]
```

Defining the control object. Here, I am chossing 5-fold cross validation as well as SMOTE to create synthetic cases for the minority class.

```{r}
ctrl  <- trainControl(method = "cv", 
                      number = 5,
                      summaryFunction = twoClassSummary,
                      verboseIter = TRUE,
                      classProbs = TRUE,
                      sampling = "smote",
                      savePredictions = "final")
```

# Train models 

## KNN 

```{r}
set.seed(28938)

knn <- train(bot ~ .,
             data = model_train,
             method = "knn",
             trControl = ctrl,
             preProcess = c("center","scale"),
             metric = "ROC")
```

```{r}
p_knn <- predict(knn, model_test, type = "raw")
confusionMatrix(p_knn, model_test$bot, mode = "everything", positive = "Yes")

prob_knn <- predict(knn, model_test, type = "prob")
```


# XGBoost 

```{r}
grid_xg <- expand.grid(max_depth = c(2, 3),
                    nrounds = c(50, 100),
                    eta = c(0.4),
                    min_child_weight = c(1,2),
                    gamma = c(0, 0.5),
                    colsample_bytree = c(0.4, 1.0),
                    subsample = c(0, 1))
```

```{r}
set.seed(79647)
xgb <- train(bot ~ .,
             data = model_train,
             method = "xgbTree",
             trControl = ctrl,
             tuneGrid = grid_xg,
             metric = "ROC")
```

```{r}
p_xgb <- predict(xgb, newdata = model_test, type = "raw")
confusionMatrix(p_xgb, model_test$bot, mode = "everything", positive = "Yes")

prob_xgb <- predict(xgb, newdata = model_test, type = "prob")
round(prob_xgb, 2)
```

## C5.0

```{r}
set.seed(87543)

c5 <- train(bot ~ .,
            data = model_train,
            method = "C5.0",
            trControl = ctrl)
```

```{r}
p_c5 <- predict(c5, model_test, type = "raw")
confusionMatrix(p_c5, model_test$bot, mode = "everything", positive = "Yes")

prob_c5 <- predict(c5, model_test, type = "prob")
bind_cols(round(prob_c5, 2), model_test$bot)

```

## Extra-Trees 

```{r}
grid_et <- expand.grid(mtry = c(2,4,6, 8, 10, 12),
                    splitrule = c("extratrees"),
                    min.node.size = c(5,10,20))
```

```{r}
set.seed(87543)
et <- train(bot ~ .,
            data = model_train,
            method = "ranger",
            trControl = ctrl,
            tuneGrid = grid_et)
```

```{r}
p_et <- predict(et, model_test, type = "raw")
confusionMatrix(p_et, model_test$bot, mode = "everything", positive = "Yes")

prob_et <- predict(et, model_test, type = "prob")
```

Creating ROC Objects. 

```{r}
xgb_roc <- roc(model_test$bot, prob_xgb$Yes)
knn_roc <- roc(model_test$bot, prob_knn$Yes)
c5_roc <- roc(model_test$bot, prob_c5$Yes)
et_roc <- roc(model_test$bot, prob_et$Yes)
history
```

Plotting ROC Objects. 

```{r}
ggroc(list(XGB = xgb_roc, 
           C5.0 = c5_roc,
           ExtraTrees = et_roc,
           KNN = knn_roc)) +
  geom_segment(aes(x = 1, xend = 0, y = 0, yend = 1), 
               color="darkgrey", linetype="dashed") +
  theme(legend.title = element_blank())
```

##### GloVe Model ########

Select relevant variables and merge text variables. 

```{r}
data_glove <- data %>% select(c(bot, feedback))
data_glove$feedback <- as.character(data_glove$feedback)
```

Clean-up bot variable.

```{r}
data_glove$bot[is.na(data_glove$bot)] <- 0
data_glove$bot[data_glove$bot == 2] <- 1
data_glove$bot[data_glove$bot == 3] <- 1
data_glove$bot <- as.integer(data_glove$bot)
```

Create train & test dataset. 

```{r}
data_train <- data_glove[1:1200,]
data_test <- data_glove[1201:1351,]
```

Run only if pre_trained vectors are not downloaded yet. 

```{r eval=FALSE, include=FALSE}
if (!file.exists('glove.6B.zip')) {
  download.file('http://nlp.stanford.edu/data/glove.6B.zip',destfile = 'glove.6B.zip')
  unzip('glove.6B.zip')
}
```

Read in vectors. 

```{r}
vectors <- data.table::fread('glove.6B.300d.txt', data.table = F,  encoding = 'UTF-8')
colnames(vectors) = c('word',paste('dim',1:300,sep = '_'))

as_tibble(vectors)
```

Set parameters.

```{r}
max_words = 1e4
maxlen = 60
dim_size = 300
```

```{r}
word_seqs <- text_tokenizer(num_words = max_words) %>% 
  fit_text_tokenizer(data_train$feedback)
```

```{r}
x_train <- texts_to_sequences(word_seqs, data_train$text) %>%
  pad_sequences( maxlen = maxlen)
```

```{r}
y_train <- as.matrix(data_train$bot)
word_indices <- unlist(word_seqs$word_index)
```

```{r}
dic <- data.frame(word = names(word_indices), key = word_indices, stringsAsFactors = FALSE) %>%
  arrange(key) %>% .[1:max_words,]
```

```{r}
word_embeds <- dic  %>% left_join(vectors) %>% .[,3:302] %>% replace(., is.na(.), 0) %>% as.matrix()
```

```{r}
input <- layer_input(shape = list(maxlen), name = "input")

model <- input %>%
  layer_embedding(input_dim = max_words, output_dim = dim_size, input_length = maxlen,
                  weights = list(word_embeds), trainable = FALSE) %>%
  layer_spatial_dropout_1d(rate = 0.2 ) %>%
  bidirectional(
    layer_gru(units = 80, return_sequences = TRUE)
  )
max_pool <- model %>% layer_global_max_pooling_1d()
ave_pool <- model %>% layer_global_average_pooling_1d()

output = layer_concatenate(list(ave_pool, max_pool)) %>%
  layer_dense(units = 1, activation = "sigmoid")

model <- keras_model(input, output)

model %>% compile(
  optimizer = "adam",
  loss = "binary_crossentropy",
  metrics = tensorflow::tf$keras$metrics$AUC()
)

history <- model %>% keras::fit(
  x_train, y_train,
  epochs = 16,
  batch_size = 32,
  validation_split = 0.2
)
```

```{r}
x_test <- texts_to_sequences(word_seqs, data_test$text) %>%
  pad_sequences( maxlen = maxlen)

y_test <- as.matrix(data_test$bot)
```

Make predictions with data from testset. 

```{r}
pred <- model %>% predict(x_test)
format(round(pred, 2), scientific = FALSE)
```

Evaluate performance. 

```{r}
model %>% evaluate(x_test, y_test, verbose = 0)
```

```{r}
newdf <- data.frame(probabilites = format(round(pred, 2)), bot = data_test$bot)
newdf
```

```{r}
auc_xgb <- xgb_roc$auc
et_auc <- et_roc$auc
knn_auc <- knn_roc$auc
c5_auc <- c5_roc$auc
glove_auc <- model %>% evaluate(x_test, y_test, verbose = 0)
```

Final comparison of AUC: Choosing the "winner"

```{r}
fin_comp <- data.frame(Model = c("XGB", "ExtraTrees", "KNN", "C5.0", "GloVe"), 
                       AUC = c(auc_xgb, et_auc, knn_auc, c5_auc, glove_auc[2]))

fin_comp %>% arrange(desc(AUC)) 
```








